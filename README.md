
I have a couple of different data sources. 
I'm looking at each one of our assets with specific data attached to them. First there is our Audience Measurement System (AMS) 'scoreboard', with different 'scores' allocaated to each face (not asset - if a scrolling sign has three frames, then our AMS measures each of these individually as faces.
Secondly there are some ranking reports that show which of these individual 'faces' reach certain, traditionally high value audiences for advertisers (e.g high income earners etc). These don't have the same marking identifiers for each face, so an indexing spreadsheet has also been uploaded to 'translate' each image.
Essentially, I'll be trying to see how these different data sources can influence a new way of pricing our assets - the initial stages of a programmatic solution. I'll be allocating different levels to each asset according to how well they deliver against certain audiences. For example, the top 20% (this figure may change) will be given a certain price premium, the second quintile will be given another price premium and so on and so forth. 

I will be then needing to compare example weeks of activity in this new pricing universe with our own current way of selling and see if the same yield is given.

At this point the data will be extremely sensitive so Ill probably have to get you all to sign an NDA (!). However this is where the modelling will be the most cruicial - although I'm actually not really sure what modelling to do at this stage. 

The current dataframes I have are all quite simply in CSV format, with the AMS scoreboard a huge dataframe, as is the index and then I'm compiling a huge list of ranking the assets in relation to a range of common advertising demographic targets. They are all pretty clean, with no NaNs. The cleansing challenge will be in matching up the assets and then attempting to create a week of activity.




```python

```
